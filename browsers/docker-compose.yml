services:

  ollama_proxy:
    build: ./lama-proxy
    container_name: ollama_proxy
    ports:
      - "11434:5000"
    environment:
      - "UPSTREAM=http://ollama:11434"
      - "NAME=lama"
      - "MONGO_HOSTNAME=mongo-cache"
    
  gepeto_proxy:
    build: ./lama-proxy
    container_name: gepeto_proxy
    ports:
      - "11333:5000"
    environment:
      - "UPSTREAM=https://api.openai.com"
      - "NAME=gepetos"
      - "MONGO_HOSTNAME=mongo-cache"
  

  python-ui:
    container_name: python-ui
    build: .
    restart: always
    ports:
      - 8888:8888
    command:  /bin/bash -c 'while true; do python ui.py || sleep 3; done'
    environment:
      - HD_HOST=0.0.0.0
      - TEMPORAL_ADDRESS=${TEMPORAL_ADDRESS:-temporal:7233}  
      - MONGO_HOSTNAME=${MONGO_HOSTNAME:-mongo-cache}  
      - SELENIUM_IP=${SELENIUM_IP:-selenium-hub}
      - IFRAME_IP
      - LLAMAINDEX_HOST=${LAMAINDEX_HOST:-ollama}
      - LLAMAINDEX_PORT=${LAMAINDEX_PORT:-5000}
      - NEO4J_HOSTNAME=${NEO4J_HOSTNAME:-neo4j}
    volumes:
      - .:/app:ro

  
  python-workers:
    container_name: python-workers
    build: .
    restart: always
    command:  /bin/bash -c 'while true; do python workers.py || sleep 8; done'
    shm_size: 5g
    mem_limit: 9g
    environment:
      - HD_HOST=0.0.0.0
      - TEMPORAL_ADDRESS=${TEMPORAL_ADDRESS:-temporal:7233}  
      - MONGO_HOSTNAME=${MONGO_HOSTNAME:-mongo-cache}  
      - SELENIUM_IP=${SELENIUM_IP:-selenium-hub}
      - IFRAME_IP
      - LLAMAINDEX_HOST=${LAMAINDEX_HOST:-ollama_proxy}
      - LLAMAINDEX_PORT=${LAMAINDEX_PORT:-5000}
      
      - LLAMAINDEX_HOST_NOPROXY=${LAMAINDEX_HOST:-ollama}
      - LLAMAINDEX_PORT_NOPROXY=${LAMAINDEX_PORT:-11434}

      - NEO4J_HOSTNAME=${NEO4J_HOSTNAME:-neo4j}
      - OPENAI_API_KEY
    volumes:
      - .:/app:ro


# volumes: 


networks:
  default:
    name: hekthon
    external: true