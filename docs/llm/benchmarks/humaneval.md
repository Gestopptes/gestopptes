[Source](https://arxiv.org/pdf/2107.03374) [Extracted with](https://chatgpt.com/share/e/e912a8ae-b266-42e7-bfde-5a3e8cccd9d9)
## HumanEval Dataset: Definition and Examples

### Introduction
The HumanEval dataset is a set of 164 hand-written programming problems designed to evaluate the functional correctness of code generated by language models. Each problem includes a function signature, a docstring, the function body, and several unit tests. This dataset is critical for benchmarking models like Codex, which are trained to generate code from natural language specifications.

### Dataset Structure
Each problem in the HumanEval dataset has the following components:
1. **Function Signature**: Defines the function's input parameters and return type.
2. **Docstring**: Provides a description of the function's purpose and the expected behavior.
3. **Body**: Contains the code implementation, which the model is supposed to generate.
4. **Unit Tests**: A set of tests to validate the correctness of the generated code, averaging 7.7 tests per problem.

### Purpose
The primary goal of the HumanEval dataset is to measure the functional correctness of code generation models. Functional correctness is evaluated based on whether the generated code passes all provided unit tests, rather than relying on match-based metrics like BLEU score, which may not fully capture the semantic accuracy of the code.

### Evaluation Metric: Pass@k
The evaluation uses the pass@k metric, where k represents the number of code samples generated per problem. A problem is considered solved if any of the k samples pass the unit tests. The pass@k metric provides a more accurate measure of a model's ability to generate functionally correct code compared to traditional metrics.

#### Pass@k Calculation
To calculate pass@k, n samples are generated for each task, where n â‰¥ k. The number of correct samples (c) that pass the unit tests is counted, and the pass@k metric is calculated using the following formula:

\[ \text{pass@k} = E_{\text{Problems}} \left[ 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}} \right] \]

This formula ensures an unbiased estimation of the pass@k metric.

### Example Problems from HumanEval
The HumanEval dataset covers a variety of programming tasks, including language comprehension, algorithms, and basic mathematics. Here are three example problems from the dataset:

#### Example 1: Simple Arithmetic
**Docstring**:
```python
"""
Write a function to add two numbers.
"""
```
**Function Signature**:
```python
def add(a: int, b: int) -> int:
```
**Unit Tests**:
```python
assert add(2, 3) == 5
assert add(-1, 1) == 0
assert add(0, 0) == 0
```

#### Example 2: String Manipulation
**Docstring**:
```python
"""
Reverse the input string.
"""
```
**Function Signature**:
```python
def reverse_string(s: str) -> str:
```
**Unit Tests**:
```python
assert reverse_string("hello") == "olleh"
assert reverse_string("abcd") == "dcba"
assert reverse_string("") == ""
```

#### Example 3: Factorial Calculation
**Docstring**:
```python
"""
Calculate the factorial of a non-negative integer.
"""
```
**Function Signature**:
```python
def factorial(n: int) -> int:
```
**Unit Tests**:
```python
assert factorial(5) == 120
assert factorial(0) == 1
assert factorial(3) == 6
```

### Results: Model Performance on HumanEval
The performance of various models on the HumanEval dataset is summarized in the following tables. Only the largest and smallest versions of each model are included.

#### Table 1: Evaluations for HumanEval
| Model         | Pass@1 | Pass@10 | Pass@100 |
|---------------|--------|---------|----------|
| GPT-Neo 125M  | 0.75%  | 1.88%   | 2.97%    |
| GPT-J 6B      | 11.62% | 15.74%  | 27.74%   |
| Codex-12M     | 2.00%  | 3.62%   | 8.58%    |
| Codex-12B     | 28.81% | 46.81%  | 72.31%   |

#### Table 2: Fine-tuned GPT-Neo Results from APPS Dataset
| Model                      | Introductory Pass@1 | Interview Pass@1 | Competition Pass@1 |
|----------------------------|---------------------|------------------|--------------------|
| GPT-Neo 2.7B Raw Pass@1    | 3.90%               | 0.57%            | 0.00%              |
| GPT-Neo 2.7B Raw Pass@5    | 5.50%               | 0.80%            | 0.00%              |
| 1-Shot Codex Raw Pass@1    | 4.14%               | 0.14%            | 0.02%              |
| 1-Shot Codex Filtered Pass@1| 22.78%              | 2.64%            | 3.04%              |

#### Table 3: Pass Rates for Codex-D
| Model        | Pass@1  | Pass@10 |
|--------------|---------|---------|
| Codex-S-12B  | 32.2%   | 59.5%   |
| Codex-D-12B  | 20.3%   | 46.5%   |

### Conclusion
The HumanEval dataset is a crucial tool for evaluating the functional correctness of code generated by language models. By providing a diverse set of programming problems with corresponding unit tests, it allows for a comprehensive assessment of a model's ability to generate correct and reliable code. The pass@k metric used in this evaluation offers a robust measure of performance, ensuring that models are tested in a way that reflects real-world coding scenarios.